# -*- coding: utf-8 -*-
"""
Created on Fri Dec 10 13:15:17 2021

@author: Jashan Preet Singh
"""

'''
Main steps taken:
1) Load data
2) Increase data using autoencoder to make stratified datasets
3) scaling data
4) Shuffling data in-folder(shuffling images among the one person images)
5) Create stratified train, validation, test data
6) Shuffle all 3 datasets again out-fold(shuffling across whole dataset that includes all persons images)
7) Implementing different cluster techniques
8) Training and evaluating ANN classfier model
9) Fine-tuning Hyper parameters of ANN model - RandomizedSearchCV
10) Predicting and showing the results
    
    Getting 92-95% Accuray on Test data(Unkown data)
'''
import numpy as np
import pandas as pd
from scipy.io import loadmat
import matplotlib.pyplot as plt
import tensorflow as tf
import keras
from keras import layers


from sklearn.model_selection import StratifiedKFold
from tensorflow import keras
from tensorflow.keras import layers


#----loading and checking the data 
loaded_data = loadmat('umist_cropped.mat')


data = loaded_data['facedat']


a = data[0]
b= data[0][0]
b1= data[0][1]


#Checking shape and total number of images in each person's directory
total=0
for i in range(len(a)):
    print(f'person {i+1}->{a[i].shape}')
    temp = a[i].shape[2]
    total = total+temp
print(f'Total images of all persons in the dataset--> {total}')








#----------------Autoencoder on every person-------#
'''
After checking the data, we found that total number of images are different for each person
Hence, using Autoencoder we are generating new but same kind of images for the person whose
total images are less then 48 which is a maximum number of image of 19th person in the dataset

So that, at the end, each person will have 48 images and total number of images will be 
increased to 48*20 = 960
'''


#Created 2-D empty array, so that we can append images to this array; that contains pixels of each image in the row
#Also, these pixels are scaled, hence value would be normalized between 0 to 1.
#Furthermore, this array will contain original images and new images generated by autoencoder where needed to finally create 
#array with 960 images that means 960 rows.
data_array = np.empty((1,10304))



for i in range(20):
    
    per = data[0][i]
    per = per/255  #Scaling
    
    total_img_per = per.shape[2] #Taking total number of images of each person
    
    #Flatten data
    temp = per.reshape(10304,total_img_per)
    per_flat = temp.T
    
    
    encoding_dim = 64 # 32 floats -> compression of factor 24.5, assuming the input is 784 floats
    
    #Encoder
    # This is our input image
    input_img = keras.Input(shape=(10304,))
    encoded = layers.Dense(300, activation='relu')(input_img)
    encoded = layers.Dense(100, activation='relu')(encoded)
        
    encoded = layers.Dense(encoding_dim, activation='relu')(encoded)    #Bottleneck layer
        
    #Decoder
    decoded = layers.Dense(100, activation='relu')(encoded)
        
    decoded = layers.Dense(300, activation='relu')(decoded)
    decoded = layers.Dense(10304, activation='sigmoid')(decoded)
   
    # This model maps an input to its reconstruction
    autoencoder = keras.Model(input_img, decoded)
        
        
        
    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), 
                                    loss='binary_crossentropy')
        
        
        
    autoencoder.fit(per_flat, per_flat,
                            epochs=20,
                            batch_size= 16,
                            shuffle=True
                            )
        
    data_concat = np.empty((1, 10304))
    while total_img_per<48:
        per_enc = autoencoder.predict(per_flat)
            
        data_concat = np.concatenate((data_concat, per_enc), axis=0)
            
        total_img_per += per_enc.shape[0]
        print(total_img_per)
        
    data_concat = data_concat[1:,:]
        
    data_array = np.concatenate((data_array, per_flat), axis=0)
    data_array = np.concatenate((data_array, data_concat[:48-per_flat.shape[0],:]), axis=0)
        
data_array = data_array[1:,:]    

#shape of the array 960X 10304 where 960 row for 960 images
#10304 is flattened features converted from 112 X 92 = 10304 dimension
print(data_array.shape)


for i in range(94,96):
    plt.imshow(data_array[i:i+1,:].reshape((112,92)))
    plt.title('Images generated by Autoencoder')
    plt.show()
    
    





#-------Creating training, validation and testing dataset--------#
'''
As we have same total number of images(48 images each) for all 20 persons in the dataset,
we can create stratified training , validation and testing data

We are taking 30 images of each person for trainig data. Hence total images for training is 30X20 = 600

From remaining images, we are diviing images into 50% - 50% for validation and test data
Hence, we are taking 9 images of each person for validation and testing data both
Thus, validation data and test data will have 9X20 = 180 images total in them.
'''

train_data = np.empty((1, 10304))
val_data = np.empty((1, 10304))
test_data = np.empty((1, 10304))

start = 0 
end = 48

while end < 961:
    main_data = data_array[start:end, :]
    main_data = np.take(main_data ,np.random.permutation(main_data.shape[0]),axis=0,out=main_data)

    
    train_data = np.concatenate((train_data,main_data[:30,:]), axis=0)
    val_data = np.concatenate((val_data,main_data[30:39,:]), axis=0)
    test_data = np.concatenate((test_data,main_data[39:48,:]), axis=0)
    
    start += 48
    end += 48
    
train_data = train_data[1:,:]
val_data = val_data[1:,:]
test_data = test_data[1:,:]





#------Shuffling training , validation and testing dataset----------#

'''
In above section, we created train ,validation and test data
But, it contains images in order starting from person 1 to person 20
With this kind of dataset, our classification model or cluster technique won't be able
to person well.
Hence, we need to shuffle all 3 datasets
'''

    
train_shuff = np.take(train_data ,np.random.permutation(train_data.shape[0]),axis=0,out=train_data)
val_shuff = np.take(val_data ,np.random.permutation(val_data.shape[0]),axis=0,out=val_data)
test_shuff = np.take(test_data ,np.random.permutation(test_data.shape[0]),axis=0,out=test_data)


    


    
#---- Visualzing images after processing---#

'''
After performing number of processing steps on dataset, let's check few images
from the newly created datasets
'''

for i in range(20,40):
    plt.imshow(train_shuff[i:i+1,:].reshape((112,92)))
    plt.show()
    




#--------Dimensionality reduction--------#

'''
Dimension of all images is = (112,92)
We have flatten this dimensions are saved all the pixels of each image in row 
Hence we have total 10304 features.

We can reduce features using PCA (Principle Component Analysis) technique
By keeping 99% or variance we will reduce dimensions of all 3 datasets

'''


from sklearn.decomposition import PCA
pca = PCA(0.99) # create a PCA object that ensures 99% variance

pca.fit(train_shuff) # do the math
train_pca = pca.transform(train_shuff) # get PCA coordinates for scaled trainig data


#Also, to visualize 2D graphs we are saving only first 2 Priniciple Component to 'train_pca2' variable
#But, we will train_pca2 only for 2D visualization. Other than that, we will use 'train_pca' variable that contains 99% variance.
train_pca2 = train_pca[:,0:2]


#--PCA on validation set
val_pca = pca.transform(val_shuff)

#--PCA on test set
test_pca = pca.transform(test_shuff)

print(f'\nDimension of training data after reducing dimension --> {train_pca.shape}')
print(f'Dimension of validation data after reducing dimension --> {val_pca.shape}')
print(f'Dimension of testing data after reducing dimension --> {test_pca.shape}')






#------Using different Clustering Techniques to generate clusters------#
'''
We have tried 3 clustering Techniques
1) AHC - agglomerative hierarchical clustering
2) K-means clustering
3) GMM - Gaussian Mixture Model

As this project has Unsupervised problem, we got train, validation and test data from given data
But, we dont have Acutal labels connected to it.

Using clustering technique, we will try to segregrate all datasets into clusters where each cluster
contains same(similar) images.

We will assume that each cluster contains images of same person and we will attach same label to all the images in the same cluster
And, this is how we will get Actual labels for all 3 dataset with which we can use any Supervised learning classifier to training model and 
give predictions
'''

#--------------------------#
#------Using AHC on PCA trainig data to and visualize and select number of clusters
#--------------------------#
import scipy.cluster.hierarchy as sch
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score


#Creating dendogram to check correct number of clusters
dnd = sch.dendrogram(sch.linkage(train_pca2, method='ward'))




# Initialize hiererchial clustering method, in order for the algorithm to determine the number of clusters
# putting n_clusters=None, compute_full_tree = True,
# best distance threshold value for this dataset is distance_threshold = 55
cluster = AgglomerativeClustering(n_clusters=None, affinity='euclidean', linkage='ward', compute_full_tree=True, 
                                  distance_threshold=54)

# Cluster the data
cluster.fit_predict(train_pca)

print(f"Number of clusters = {1+np.amax(cluster.labels_)}")
AHC_labels = cluster.labels_

#Plotting
plt.scatter(train_pca2[:,0],train_pca2[:,1], c=cluster.labels_, cmap='rainbow')
plt.title(f"AHC estimated number of clusters = {1+np.amax(cluster.labels_)}")
plt.show()


#Above, we tried to affinity = euclidean, Manhattan, cosine 
#And, tried linkage = ward, complete, single, average
#However, combination of 'euclidean' and 'ward' seems to be giving best seggregation of clusters





#-------------------Kmeans clustering-------------#
'''
K-means clustering does hard clustering
It tends to create rounded shaped cluster and tries to keep cluster sizes same
It works on cluster centers. Hence it is a center based clustering technique
Also, it is a top-down clustering technique
'''
from sklearn.cluster import KMeans


#Choosing K-value for k-mean cluster - using origina features
sse = []
k_list = range(2, 40)
for k in k_list:
    km = KMeans(n_clusters=k)
    km.fit(train_pca)
    sse.append([k, km.inertia_])

#Creating a line graph to select best k-value
oca_results_scale = pd.DataFrame({'Cluster': range(2,40), 'SSE': sse})
plt.figure(figsize=(12,6))
plt.plot(pd.DataFrame(sse)[0], pd.DataFrame(sse)[1], marker='o')
plt.title('Optimal Number of Clusters using Elbow Method (Scaled Data)')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')


#Selected k-value =20

kmeans_scale = KMeans(n_clusters=20, n_init=100, max_iter=400, init='k-means++', random_state=42).fit(train_pca)
print('KMeans Scaled Silhouette Score: {}'.format(silhouette_score(train_pca, kmeans_scale.labels_, metric='euclidean')))
 
labels_scale = kmeans_scale.labels_


plt.scatter(train_pca2[:,0],train_pca2[:,1], c=kmeans_scale.labels_, cmap='rainbow')
plt.title(f"K-means estimated number of clusters = {1+np.amax(kmeans_scale.labels_)}")
plt.show()



#--------Gaussian Mixture Model---------#
'''
GMM creates clusters on the basis of Gaussian Mixture.
Hence, it add data point to the cluster on the basis of mean and variance of the Gaussian Mixture
It can do both soft and hard clustering.
It is more flexible than K-means clustering
'''

from sklearn.mixture import GaussianMixture


gmm_vis = GaussianMixture(20, covariance_type='full', random_state=0)
gmm_vis.fit(train_pca2)



from matplotlib.colors import LogNorm

# display predicted scores by the model as a contour plot
x = np.linspace(-20.0, 20.0)
y = np.linspace(-20.0, 20.0)
X, Y = np.meshgrid(x, y)
XX = np.array([X.ravel(), Y.ravel()]).T
Z = -gmm_vis.score_samples(XX)
Z = Z.reshape(X.shape)

CS = plt.contour(
    X, Y, Z, norm=LogNorm(vmin=1.0, vmax=1000.0), levels=np.logspace(0, 3, 10)
)
CB = plt.colorbar(CS, shrink=0.8, extend="both")
plt.scatter(train_pca2[:, 0], train_pca2[:, 1], 0.8)

plt.title("Negative log-likelihood predicted by a GMM")
plt.axis("tight")
plt.show()



from matplotlib.patches import Ellipse

def draw_ellipse(position, covariance, ax=None, **kwargs):
    """Draw an ellipse with a given position and covariance"""
    ax = ax or plt.gca()
    
    # Convert covariance to principal axes
    if covariance.shape == (2, 2):
        U, s, Vt = np.linalg.svd(covariance)
        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))
        width, height = 2 * np.sqrt(s)
    else:
        angle = 0
        width, height = 2 * np.sqrt(covariance)
    
    # Draw the Ellipse
    for nsig in range(1, 4):
        ax.add_patch(Ellipse(position, nsig * width, nsig * height,
                             angle, **kwargs))
        
def plot_gmm(gmm, X, label=True, ax=None):
    ax = ax or plt.gca()
    labels = gmm.fit(X).predict(X)
    if label:
        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)
    else:
        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)
        
    ax.axis('equal')
    plt.title('GMM estimated number of clusters = 20')    
    w_factor = 0.2 / gmm.weights_.max()
    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):
        draw_ellipse(pos, covar, alpha=w * w_factor)



plot_gmm(gmm_vis, train_pca2)


#Checked GMM model with different 'Covariance_type' = full, tied, diag, spherical
#Found that 'full' was giving best results because it was giving less skewed labels and 
#it was giving more accuracy at the end on test data


'''
Selected Gaussian Mixture Model as a clustering Technique for this problem
Reasons:
1) By analyzing graph, it seems that many data are scatter on ellipsodial shape.
    GMM works better when clusters shape are ellipsoial because it used gaussian mixture and covariance rather than 
    only depending on center of cluster

2) As it was generating less skewed data and it was generating as much as same number of labels for each person.
    We are looking for same number of labels for each person because we already know that same number of images are present 
    in trainig data.
    
3) By running classifier in following section; using labels generated by all clustering technique we found that 
    classifier was giving bit high accuracy consistantly when we used labls generated by GMM.

'''

#Therefore, we are generating labels using GMM for all 3 datasets
#Labels generated by GMM will act as actual labels for correspnding features dataset
#And, we will be able to train supervised learning model on the basis of training data that we created above 
#and correspnding actual labels that are generated below by GMM technique

#Fitting on train_pca  
gmm = GaussianMixture(20, covariance_type='full', random_state=2)
gmm.fit(train_pca)

#--Creating trainig Actual labels
train_labels = gmm.predict(train_pca)

#---Creating validation Acutal labels
val_labels= gmm.predict(val_pca)

#--Creating testing Actual labels
test_labels= gmm.predict(test_pca)



#------ANN(Artificial Neural Network)--------#

'''
As we have trainig features in 'train_pca' and its actual labels in 'test_labels' variable
We can train ANN model to make predictions
'''

import tensorflow as tf
from tensorflow.keras import datasets, layers, models
from tensorflow import keras
from sklearn.metrics import accuracy_score
from sklearn.model_selection import RandomizedSearchCV

'''
Input layer contains total nodes equal to toal number of features(columns) in training data
Out Layer contains total nodes equal to toal number of class we have in labels

Total number of hidden layers = 3
Total Nodes in the hidden layers respectively = 3000, 1000, 500

Activation function in hidden layer = LeakyRelu
Activation function in output layer = Softmax

'''
ann = models.Sequential([
    keras.Input(shape=(train_pca.shape[1],)),
    layers.Dense(3000, activation=keras.layers.LeakyReLU(alpha=0.01) ),
    layers.Dense(1000, activation=keras.layers.LeakyReLU(alpha=0.01)),
    layers.Dense(500, activation=keras.layers.LeakyReLU(alpha=0.01)),
    layers.Dense(20, activation='softmax')
    
    ])


'''
Used loss function 'sparse_categorical_crossentropy' because we have multiclass classification problem.
Hence we can not use 'binary_crossentropy' as loss function

Tried optimizer adam and SGD. But 'adam' given high accuracy than SGD
'''
ann.compile(optimizer='adam' , loss='sparse_categorical_crossentropy', metrics=['accuracy'])


ann.fit(train_pca, train_labels, epochs=30, validation_data = (val_pca,val_labels))


ann.evaluate(test_pca, test_labels)


#Predicting on Test data(unkown data)
y_pred = ann.predict(test_pca)

for i in range(y_pred.shape[0]):
    y_pred[i] = y_pred[i].argmax()
    
y_pred = y_pred[:,:1]
    
#Accuracy
accuracy_score(y_pred,test_labels )




'''
Fine- tuning the hyper parameters to select best model with best parameter
'''
# Created function that helps to finetune 3 hyperparameters
# Number of hidden layers
# Number of neurons in hidden layers
# Value of learning rate

def build_model(n_hidden=1, n_neurons=3000, opt='adam'):
    
    model = keras.models.Sequential()
    model.add(keras.Input(shape=(train_pca.shape[1],)))
    for layer in range(n_hidden):
        model.add(keras.layers.Dense(n_neurons, activation=keras.layers.LeakyReLU(alpha=0.01)))
    
    model.add(keras.layers.Dense(20,activation='softmax'))
    optimizer = opt
    model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
    return model



#Created dictionary that contain different value of hyper-parameter from which we want to find best using fine-tuning
param_distribs = {
"n_hidden": [2,3],
"n_neurons": [1000,3000],
"opt": ['adam', 'sgd'],
}

#Created object of KerasClassifier so that we can use randomizedSearchCV to fine-tune on Keras object
keras_reg = keras.wrappers.scikit_learn.KerasClassifier(build_fn=build_model,epochs=5,batch_size=16)


#Created object of RandomizedSearchCV
np.random.seed(42)
rscv = RandomizedSearchCV(estimator= keras_reg, param_distributions=param_distribs, cv=3,  n_iter=5)

#Fitted training data to train model
rscv_results = rscv.fit(train_pca,train_labels, validation_data=(val_pca,val_labels))



#Best paramters after fine-tuning model
rscv.best_params_


#Predicted test data
y_pred = rscv.predict(test_pca)

accuracy_score(y_pred, test_labels)



#Plotting images 
#Their titles contains actual labels and predicts results
for i in range(100,110):
    plt.imshow(test_shuff[i:i+1,:].reshape((112,92)))
    plt.title(f'Actual Label - {test_labels[i]}, pred label - {y_pred[i]}')
    plt.show()

